{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c92f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wave\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import audioread\n",
    "from pydub import AudioSegment\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import librosa.display\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import ast\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import Input,Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.train import Checkpoint,CheckpointManager\n",
    "path= 'D:\\\\Work\\\\Speaker classification\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d75ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reading_audio_and_removing_silence():\n",
    "    \n",
    "    def __init__(self,path_audio_dir,path_csv_file,sample_rate=22050):\n",
    "    \n",
    "        self.dir_audio= path_audio_dir\n",
    "        self.dir_csv_file= path_csv_file\n",
    "        self.sub_path= None\n",
    "        self.sample_rate= sample_rate\n",
    "        \n",
    "        #Reading the padnas self.dataframe\n",
    "        self.dataframe= pd.read_csv(self.dir_csv_file)\n",
    "        self.info= {}\n",
    "\n",
    "        #Droping the nan values from these rows\n",
    "        self.dataframe= self.dataframe[self.dataframe['Source']=='GOV']\n",
    "        self.dataframe.dropna(axis=0,subset= ['Name','Link'],inplace=True)\n",
    "        \n",
    "        #include the path of the audio in the self.dataframe\n",
    "        self.dataframe['path']= [self.dir_audio+Name+'.MP3' for Name in self.dataframe['Name'].values]\n",
    "\n",
    "\n",
    "    def creating_a_dataframe(self,min_silence):   \n",
    "        dataset= pd.DataFrame(columns=['Name','Gender','Features','Duration'])\n",
    "        all_feature= []\n",
    "        \n",
    "        if self.sub_path==None:\n",
    "            \n",
    "            if self.dir_audio.split('\\\\')[-1]=='':\n",
    "                self.sub_path= '\\\\'.join(self.dir_audio.split('\\\\')[:len(self.dir_audio.split('\\\\'))-2])\n",
    "\n",
    "            else:\n",
    "                self.sub_path= '\\\\'.join(self.dir_audio.split('\\\\')[:len(self.dir_audio.split('\\\\'))-1])\n",
    "\n",
    "        #Getting the self.dataframe\n",
    "        print('Creating an self.dataframe and removing the silence.....\\n')\n",
    "\n",
    "        for name in tqdm(self.dataframe['Name'].values):\n",
    "            feature,duration= self.reading_a_file(name)\n",
    "\n",
    "            #removing silence from the audio\n",
    "            feature= self.remove_silence(feature,min_silence)\n",
    "\n",
    "            self.info['Features']= feature\n",
    "            self.info['Duration']= duration\n",
    "            all_feature.append(feature)\n",
    "            self.info['Gender']= self.dataframe[self.dataframe['Name']==name]['Sex'].values[0]\n",
    "            self.info['Name']= name\n",
    "\n",
    "            dataset= dataset.append(self.info,ignore_index=True)\n",
    "\n",
    "        #saving the self.dataframe to the disk\n",
    "        dataset.to_csv(self.sub_path+r'\\features_dataset.csv')\n",
    "\n",
    "        with open(self.sub_path+r'\\features','wb') as f:\n",
    "            pickle.dump(all_feature,f)\n",
    "\n",
    "        return  \n",
    "    \n",
    "    \n",
    "    def getduration(self,temp):\n",
    "        #converting time 10:30 format to total number of seconds\n",
    "        minute,second= temp.split(':')\n",
    "        duration= int(minute)*60+int(second)\n",
    "        return duration\n",
    "\n",
    "    \n",
    "    \n",
    "    def reading_a_file(self,name):\n",
    "        \n",
    "        path= self.dataframe[self.dataframe['Name']==name]['path'].values[0]\n",
    "        arr_audio,_=librosa.load(path,sr= self.sample_rate)\n",
    "        \n",
    "        if len(arr_audio.shape)>1: #dual channel audio\n",
    "            #arr_audio.shape= (2,n) where 2 represent dual channel value\n",
    "            arr_audio= arr_audio.sum(0)/2\n",
    "            \n",
    "\n",
    "        #Removing noises from the audio\n",
    "        time_stamp= self.dataframe[self.dataframe['Name']==name]['Time Stamp'].values[0]\n",
    "        \n",
    "        if isinstance(time_stamp,str):\n",
    "\n",
    "            if time_stamp=='Remove':\n",
    "                #Removing first and last 1 minutes of audio\n",
    "                arr= arr_audio[self.sample_rate*60:arr_audio.shape[0]-self.sample_rate*60]\n",
    "                duration= arr//self.sample_rate\n",
    "                \n",
    "            elif len(time_stamp.split('-'))>1:\n",
    "                start,end= time_stamp.split('-')\n",
    "\n",
    "                #converting 10:40 time to seconds\n",
    "                start_duration= self.getduration(start)\n",
    "                end_duration= self.getduration(end)\n",
    "                arr= arr_audio[self.sample_rate*start_duration:self.sample_rate*end_duration]\n",
    "                duration= end_duration-start_duration\n",
    "\n",
    "            else: \n",
    "                arr= arr_audio\n",
    "                duration= arr//self.sample_rate\n",
    "\n",
    "        return arr,duration\n",
    "\n",
    "    \n",
    "    def remove_silence(self,array_feature,min_silence):\n",
    "        '''\n",
    "        array_feature:array containing amplitude of an audio\n",
    "        min_silence:threshould below which the silence will be removed\n",
    "        '''\n",
    "        \n",
    "        #Step1:Create an audio_segment\n",
    "        feature= self.float_to_int(array_feature)\n",
    "        audio = AudioSegment(feature.tobytes(),frame_rate = self.sample_rate,sample_width = feature.dtype.itemsize\\\n",
    "                             ,channels = 1)\n",
    "        \n",
    "        #Step2:removing the silence from the audio segment\n",
    "        audio_chunks = split_on_silence(audio,min_silence_len=min_silence,silence_thresh= -30,keep_silence= 100)\n",
    "\n",
    "        #Step3:converting it back to the array\n",
    "        feature= sum(audio_chunks)\n",
    "        feature= np.array(feature.get_array_of_samples())\n",
    "        feature= self.int_to_float(feature)\n",
    "        return feature\n",
    "        \n",
    "        \n",
    "#Citation:https://github.com/huseinzol05/malaya-speech/blob/master/malaya_speech/utils/astype.py\n",
    "    def float_to_int(array, type=np.int16):\n",
    "\n",
    "        if array.dtype == type:\n",
    "            return array\n",
    "\n",
    "        if array.dtype not in [np.int16, np.int32, np.int64]:\n",
    "            if np.max(np.abs(array)) == 0:\n",
    "                array[:] = 0\n",
    "                array = type(array * np.iinfo(type).max)\n",
    "            else:\n",
    "                array = type(array / np.max(np.abs(array)) * np.iinfo(type).max)\n",
    "        return array\n",
    "\n",
    "\n",
    "    def int_to_float(array, type=np.float32):\n",
    "\n",
    "        if array.dtype == type:\n",
    "            return array\n",
    "\n",
    "        if array.dtype not in [np.float16, np.float32, np.float64]:\n",
    "            if np.max(np.abs(array)) == 0:\n",
    "                array = array.astype(np.float32)\n",
    "                array[:] = 0\n",
    "            else:\n",
    "                array = array.astype(np.float32) / np.max(np.abs(array))\n",
    "\n",
    "        return array\n",
    "    \n",
    "    def __call__(self,min_silence):\n",
    "        start= datetime.now()\n",
    "        self.creating_a_dataframe(min_silence)\n",
    "        print('The time requires to read an audio as array and remove the silence is:',datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a39a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an self.dataframe and removing the silence.....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsba\\anaconda3\\envs\\newGPU\\lib\\site-packages\\librosa\\core\\audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "  0%|          | 0/34 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'reading_audio_and_removing_silence' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-526514fe5a39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreading_audio\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mreading_audio_and_removing_silence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\Work\\\\Speaker classification\\\\Audio data\\\\AUDIO\\\\'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m'D:\\\\Work\\\\Speaker classification\\\\data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreading_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_silence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a9b05ed8c2c5>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, min_silence)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_silence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreating_a_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_silence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The time requires to read an audio as array and remove the silence is:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a9b05ed8c2c5>\u001b[0m in \u001b[0;36mcreating_a_dataframe\u001b[1;34m(self, min_silence)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m#removing silence from the audio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mfeature\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_silence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_silence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a9b05ed8c2c5>\u001b[0m in \u001b[0;36mremove_silence\u001b[1;34m(self, array_feature, min_silence)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m#Step1:Create an audio_segment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mfeature\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         audio = AudioSegment(feature.tobytes(),frame_rate = self.sample_rate,sample_width = feature.dtype.itemsize\\\n\u001b[0;32m    113\u001b[0m                              ,channels = 1)\n",
      "\u001b[1;32m<ipython-input-3-a9b05ed8c2c5>\u001b[0m in \u001b[0;36mfloat_to_int\u001b[1;34m(array, type)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfloat_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'reading_audio_and_removing_silence' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "reading_audio= reading_audio_and_removing_silence('D:\\\\Work\\\\Speaker classification\\\\Audio data\\\\AUDIO\\\\' ,'D:\\\\Work\\\\Speaker classification\\\\data.csv')\n",
    "reading_audio(min_silence=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48938690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getting_audio_dataset():\n",
    "    \n",
    "    def __init__(self,path_audio_dir,path_csv_file,second,sample_rate=22050,window_size=512):\n",
    "        '''\n",
    "        window_size: duration at which the short time fourier transform is calculated\n",
    "        for speech recognition its default value is 502 at sampling rate 22050 which \n",
    "        corresponds to 23 miliseconds\n",
    "        \n",
    "        path_audio_dir:path to all the audio file\n",
    "        path_csv_file: path to the csv_file\n",
    "        sample_rate: rate at which the audio is going to be sample\n",
    "        seconds: duration during which the spectrogram is going to be computed \n",
    "        if 3 seconds than return spectrogram of 3 second of audio\n",
    "        '''\n",
    "        \n",
    "        self.sample_rate= sample_rate\n",
    "        self.window= window_size\n",
    "        self.second= second\n",
    "        self.dir_audio= path_audio_dir\n",
    "        self.dir_csv_file= path_csv_file\n",
    "        self.image_path= []\n",
    "        self.image_dir= None\n",
    "  \n",
    "        \n",
    "        #Reading the padnas self.dataframe\n",
    "        self.dataframe= pd.read_csv(self.dir_csv_file)\n",
    "        self.info= {}\n",
    "\n",
    "        #Droping the nan values from these rows\n",
    "        self.dataframe= self.dataframe[self.dataframe['Source']=='GOV']\n",
    "        self.dataframe.dropna(axis=0,subset= ['Name','Link'],inplace=True)\n",
    "        \n",
    "        #include the path of the audio in the self.dataframe\n",
    "        self.dataframe['path']= [self.dir_audio+Name+'.MP3' for Name in self.dataframe['Name'].values]\n",
    "        \n",
    "\n",
    "        if self.dir_audio.split('\\\\')[-1]=='':\n",
    "            self.sub_path= '\\\\'.join(self.dir_audio.split('\\\\')[:len(self.dir_audio.split('\\\\'))-2])\n",
    "\n",
    "        else:\n",
    "            self.sub_path= '\\\\'.join(self.dir_audio.split('\\\\')[:len(self.dir_audio.split('\\\\'))-1])\n",
    "\n",
    "\n",
    "#Step1: Creating a features self.dataframe by sampling the audios       \n",
    "    def reading_dataframe(self):   \n",
    "        '''\n",
    "        reading the saved dataframe from the disk\n",
    "        '''\n",
    "        print('Reading the dataframe from the disk...\\n')\n",
    "        dataframe= pd.read_csv(self.sub_path+r'\\features_dataset.csv')\n",
    "        dataframe.drop(axis=1,columns='Unnamed: 0',inplace=True)\n",
    "\n",
    "        with open(self.sub_path+r'\\features','rb') as f:\n",
    "            features= pickle.load(f)\n",
    "\n",
    "        dataframe['Features']= features\n",
    "        return  dataframe\n",
    "    \n",
    "#Step2:creating an spectrogram of the array which contains amplitude    \n",
    "    def spectrogram(self,segment,path):\n",
    "        #Getting the spectrogram of the audio segment\n",
    "\n",
    "        mel_spec= librosa.feature.melspectrogram(y=segment, sr=self.sample_rate, n_mels=64,n_fft= self.window)\n",
    "        \n",
    "        #Converting applitude to Decibal\n",
    "        mel_spec_dB = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        #Saving the spectrogram in the disk\n",
    "        fig = plt.Figure()\n",
    "        canvas = FigureCanvas(fig)\n",
    "        ax = fig.add_subplot()\n",
    "        p = librosa.display.specshow(mel_spec_dB, ax=ax, y_axis='log', x_axis='time')\n",
    "        image_path= self.sub_path+''\n",
    "        fig.savefig(path)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def save_spectrogram(self,dataframe,train=True):\n",
    "        \n",
    "        empty_dataframe=pd.DataFrame(columns=['id','audio1_path','audio2_path','label'])\n",
    "        \n",
    "        name= 'train' if train==True else 'test'\n",
    "        print('Creating the spectrogram of ',name)\n",
    "        if os.path.isdir(self.sub_path+r'\\images')==False:\n",
    "            os.mkdir(self.sub_path+r'\\images')\n",
    "\n",
    "        if self.image_dir==None:\n",
    "            self.image_dir= self.sub_path+r'\\images'\n",
    "\n",
    "        if os.path.isdir(self.sub_path+'\\\\images\\\\'+name)==False:\n",
    "            os.mkdir(self.sub_path+'\\\\images\\\\'+name)\n",
    "\n",
    "        for i in tqdm(range(dataframe.shape[0])):\n",
    "            audio1= dataframe['audio1'].values[i]\n",
    "            audio2= dataframe['audio2'].values[i]\n",
    "            audio_id= dataframe['id'].values[i]\n",
    "            label= dataframe['label'].values[i]\n",
    "            \n",
    "            #Saving an image\n",
    "            for j in range(2):\n",
    "                #Getting the image path\n",
    "                if j==0:\n",
    "                    image_path1= self.image_dir+'\\\\'+name+'\\\\audio1_'+str(audio_id)\n",
    "                    self.spectrogram(audio1,image_path1)\n",
    "                else:\n",
    "                    image_path2= self.image_dir+'\\\\'+name+'\\\\audio2_'+str(audio_id)\n",
    "                    self.spectrogram(audio2,image_path2)\n",
    "                \n",
    "            empty_dataframe= empty_dataframe.append({'id':audio_id,'audio1_path':image_path1,'audio2_path':image_path2\\\n",
    "                                                        ,'label':label},ignore_index=True)\n",
    "\n",
    "        return empty_dataframe\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        '''\n",
    "        Splitting the audio in train(80%) and test(20%)\n",
    "        '''\n",
    "        \n",
    "        dataframe= self.reading_dataframe()\n",
    "        train_features_dataframe= pd.DataFrame(columns=['Name','Gender','Features','Duration'])\n",
    "        test_features_dataframe= pd.DataFrame(columns=['Name','Gender','Features','Duration'])\n",
    "\n",
    "        \n",
    "        #Getting 20% of audio in test dataset\n",
    "        \n",
    "        print('Creating the train and test self.dataframe')\n",
    "        for val in dataframe.values:\n",
    " \n",
    "            \n",
    "            train_features= val[2][:int(val[2].shape[0]*0.8)]\n",
    "            train_duration= train_features.shape[0]//self.sample_rate\n",
    "            \n",
    "            test_features= val[2][int(val[2].shape[0]*0.8):]\n",
    "            test_duration= test_features.shape[0]//self.sample_rate\n",
    "            \n",
    "            train_features_dataframe= train_features_dataframe.append({'Name':val[0],'Gender':val[1],'Features':train_features\\\n",
    "                                                            ,'Duration':train_duration},ignore_index=True)\n",
    "            \n",
    "            test_features_dataframe= test_features_dataframe.append({'Name':val[0],'Gender':val[1],'Features':test_features\\\n",
    "                                                            ,'Duration':test_duration},ignore_index=True)\n",
    "       \n",
    "        return train_features_dataframe,test_features_dataframe\n",
    "\n",
    "    \n",
    "    def similar_pairs(self,num_of_similar_pair,dataframe,train=True):\n",
    "        '''\n",
    "        Getting the similar pairs of audio segment\n",
    "        '''\n",
    "        if train==True:\n",
    "            self.train= self.train.append(self.similar(dataframe,num_of_similar_pair,train),ignore_index=True)    \n",
    "        else:\n",
    "            self.test= self.test.append(self.similar(dataframe,num_of_similar_pair,train),ignore_index=True) \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def disimilar_pairs(self,num_of_disimilar_pair,dataframe,train=True):\n",
    "        #Getting the disimilar pairs of audio segment\n",
    "        if train==True:\n",
    "            self.train= self.train.append(self.disimilar(dataframe,num_of_disimilar_pair,train),ignore_index=True)    \n",
    "        else:\n",
    "            self.test= self.test.append(self.disimilar(dataframe,num_of_disimilar_pair,train),ignore_index=True)  \n",
    "        return\n",
    "\n",
    "    \n",
    "    def similar(self,dataframe,num_of_similar_pair,train=True):\n",
    "        \n",
    "        num_of_similar_pair= int(num_of_similar_pair*0.2) if train==False else num_of_similar_pair\n",
    " \n",
    "        #Creating an empty self.dataframe\n",
    "        empty_dataframe= pd.DataFrame(columns=['audio1','audio2','label'])\n",
    "        for row in dataframe.values:\n",
    "\n",
    "            #Creating hundred audio sample using audio of an individual speaker\n",
    "            for i in range(num_of_similar_pair):    \n",
    "                \n",
    "                for j in range(2):\n",
    "         \n",
    "                    #Getting the starting index for that audio\n",
    "                    start= np.random.randint(low=0,high= row[2].shape[0]-self.second*self.sample_rate)\n",
    "\n",
    "                    #Getting an segment of audio\n",
    "                    if j==0:\n",
    "                        audio_segment1= (row[2][start:start+self.second*self.sample_rate])\n",
    "                    else:\n",
    "                        audio_segment2= (row[2][start:start+self.second*self.sample_rate])\n",
    "\n",
    "                empty_dataframe= empty_dataframe.append({'audio1':audio_segment1\\\n",
    "                                               ,'audio2':audio_segment2,'label':1},ignore_index=True)\n",
    "\n",
    "        \n",
    "        return empty_dataframe\n",
    "            \n",
    "    \n",
    "    \n",
    "    def disimilar(self,dataframe,num_of_disimilar_pairs,train=True):\n",
    "        \n",
    "        num_of_disimilar_pairs= int(num_of_disimilar_pairs*0.2) if train==False else num_of_disimilar_pairs\n",
    "        \n",
    "        #Creating an empty self.dataframe\n",
    "        empty_dataframe= pd.DataFrame(columns=['audio1','audio2','label'])\n",
    "        \n",
    "        for i in range(dataframe.shape[0]):\n",
    "            other_audios_name= list(set(dataframe['Name'].values)\\\n",
    "                                    -set([dataframe['Name'].values[j] for j in range(dataframe['Name'].shape[0]) if j>i]))\n",
    "           \n",
    "            if len(other_audios_name)== dataframe.shape[0]:\n",
    "                   return empty_dataframe\n",
    "            \n",
    "            for other_name in other_audios_name:\n",
    "                    \n",
    "                for j in range(num_of_disimilar_pairs): \n",
    "                    for k in range(2):   \n",
    "                        \n",
    "                        \n",
    "                         #Getting an audio segment from one speaker\n",
    "                        if k==0:\n",
    "                            start= np.random.randint(low=0,high= dataframe['Features'].values[i].shape[0]-self.second*self.sample_rate)\n",
    "                            audio_segment1= (dataframe['Features'].values[i][start:start+self.second*self.sample_rate])\n",
    "\n",
    "                            \n",
    "                        #Getting an audio segment from other speaker\n",
    "                        else:\n",
    "                            Features= dataframe[dataframe['Name']==other_name]['Features'].values[0]\n",
    "      \n",
    "                            start= np.random.randint(low=0,high=Features.shape[0]-self.second*self.sample_rate)\n",
    "                            audio_segment2= (Features[start:start+self.second*self.sample_rate])\n",
    "                    \n",
    "                    #After getting the audio segment from two different spearker save it\n",
    "                    empty_dataframe= empty_dataframe.append({'audio1':audio_segment1\\\n",
    "                                               ,'audio2':audio_segment2,'label':0},ignore_index=True)\n",
    "\n",
    " \n",
    "    def creating_data(self,num_of_similar_pair,num_of_disimilar_pair,dataframe,use_previous=True,train=True):\n",
    "\n",
    "        \n",
    "        if train==True:\n",
    "            \n",
    "            #Creating the training dataset\n",
    "            if os.path.isfile(self.sub_path+r'\\train.csv')==False or use_previous== False:\n",
    "                \n",
    "                print('Creating the training dataset...')\n",
    "                \n",
    "                #Getting the similar pair\n",
    "                self.similar_pairs(num_of_similar_pair,dataframe,train)\n",
    "                \n",
    "                #Getting the disimilar pair\n",
    "                self.disimilar_pairs(num_of_disimilar_pair,dataframe,train)\n",
    "\n",
    "        else:\n",
    "            #Creating the testing dataset\n",
    "            if os.path.isfile(self.sub_path+r'\\test.csv')==False or use_previous== False:\n",
    "                                   \n",
    "                print('Creating the testing dataset...')\n",
    "                self.similar_pairs(num_of_similar_pair,dataframe,train)\n",
    "                self.disimilar_pairs(num_of_disimilar_pair,dataframe,train)         \n",
    "\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __call__(self,num_of_similar_pair,num_of_disimilar_pair,get_spectrogram=False,use_previous= True):\n",
    "        \n",
    "        start= datetime.now()\n",
    "        self.train= pd.DataFrame(columns=['audio1','audio2','label'])\n",
    "        self.test= pd.DataFrame(columns=['audio1','audio2','label'])\n",
    "\n",
    "        '''\n",
    "        num_of_similar_pair: \n",
    "        num_of_disimilar_pair:\n",
    "        '''\n",
    "        train_features_dataframe,test_features_dataframe= self.train_test_split()\n",
    "        \n",
    "        self.creating_data(num_of_similar_pair,num_of_disimilar_pair,train_features_dataframe,use_previous,train=True)      \n",
    "        self.creating_data(num_of_similar_pair,num_of_disimilar_pair,test_features_dataframe,use_previous,train=False)\n",
    "        \n",
    "        self.train['id']= [i for i in range(self.train.shape[0])]\n",
    "        self.test['id']=  [i for i in range(self.test.shape[0])]\n",
    "        print('\\nThe total time requires to get train test dataset is :',datetime.now()-start)\n",
    "        \n",
    "        if get_spectrogram==True:\n",
    "            print('*******************************************************************\\n')\n",
    "            start= datetime.now()\n",
    "            self.train= self.save_spectrogram(self.train,train=True)\n",
    "            print('The spectrogram of train is saved')\n",
    "            self.test= self.save_spectrogram(self.test,train=False)\n",
    "            print('The spectrogram of test is saved')\n",
    "            print('\\nThe total time requires to save all the spectrogram is :',datetime.now()-start)\n",
    "            \n",
    "            self.train.to_csv(self.sub_path+r'\\train.csv',index=False)\n",
    "            self.test.to_csv(self.sub_path+r'\\test.csv',index=False)\n",
    "    \n",
    "        return \n",
    "        \n",
    "        def tripot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e2ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the dataframe from the disk...\n",
      "\n",
      "Creating the train and test self.dataframe\n",
      "Creating the training dataset...\n",
      "Creating the testing dataset...\n",
      "\n",
      "The total time requires to get train test dataset is : 0:04:56.407567\n",
      "*******************************************************************\n",
      "\n",
      "Creating the spectrogram of  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59500 [00:00<?, ?it/s]C:\\Users\\itsba\\anaconda3\\envs\\newGPU\\lib\\site-packages\\librosa\\display.py:974: MatplotlibDeprecationWarning: The 'basey' parameter of __init__() has been renamed 'base' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n",
      "  scaler(mode, **kwargs)\n",
      "C:\\Users\\itsba\\anaconda3\\envs\\newGPU\\lib\\site-packages\\librosa\\display.py:974: MatplotlibDeprecationWarning: The 'linthreshy' parameter of __init__() has been renamed 'linthresh' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n",
      "  scaler(mode, **kwargs)\n",
      "C:\\Users\\itsba\\anaconda3\\envs\\newGPU\\lib\\site-packages\\librosa\\display.py:974: MatplotlibDeprecationWarning: The 'linscaley' parameter of __init__() has been renamed 'linscale' since Matplotlib 3.3; support for the old name will be dropped two minor releases later.\n",
      "  scaler(mode, **kwargs)\n",
      "100%|██████████| 59500/59500 [6:04:01<00:00,  2.72it/s]        \n",
      "  0%|          | 1/11900 [00:00<25:19,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spectrogram of train is saved\n",
      "Creating the spectrogram of  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11900/11900 [24:55<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spectrogram of test is saved\n",
      "\n",
      "The total time requires to save all the spectrogram is : 6:28:57.485044\n"
     ]
    }
   ],
   "source": [
    "dataset=getting_audio_dataset(path+'Dataset\\\\',path+'data.csv',second=3)\n",
    "dataset(num_of_similar_pair=100,num_of_disimilar_pair=100,get_spectrogram=True,use_previous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386556c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class creating_batch:\n",
    "    \n",
    "    def __init__(self,path_to_train_csv,path_to_test_csv,height,width,batch):\n",
    "        '''Argument:\n",
    "        batch:batch size\n",
    "        path_to_train_csv:path to the train csv file which contains path of pair of image with their correspoing labels\n",
    "        '''\n",
    "        \n",
    "        self.batch= batch\n",
    "        self.path_to_train_csv= path_to_train_csv\n",
    "        self.path_to_test_csv= path_to_test_csv\n",
    "        self.height= height\n",
    "        self.width= width\n",
    "        \n",
    "    def read(self,path,label):\n",
    "        # 1)reading the content of the path\n",
    "        content1= tf.io.read_file(path[0])\n",
    "        content2= tf.io.read_file(path[1])\n",
    "        \n",
    "        # 2)decoding the content\n",
    "        image1= tf.io.decode_png(content1,channels=3)\n",
    "        image1= tf.image.convert_image_dtype(image1, tf.float32)\n",
    "        \n",
    "\n",
    "        image2= tf.io.decode_png(content2,channels=3)\n",
    "        image2= tf.image.convert_image_dtype(image2, tf.float32)\n",
    "        \n",
    "        #Reshazing the image\n",
    "        image1= tf.image.resize(image1,[self.height,self.width])\n",
    "        image2= tf.image.resize(image2,[self.height,self.width])\n",
    "        \n",
    "\n",
    "        \n",
    "        #3) Normalizing the image\n",
    "#         image1= preprocessing.Normalization()(image1)\n",
    "#         image2= preprocessing.Normalization()(image2)\n",
    "        \n",
    "        return image1,image2,label\n",
    "\n",
    "    \n",
    "    def pipeline(self,paths,label):\n",
    "\n",
    "        dataset=tf.data.Dataset.from_tensor_slices((paths,label))\n",
    "       #reading the images.....................................................\n",
    "        dataset= dataset.map(self.read,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "       #Creating the batch\n",
    "        dataset= dataset.batch(batch_size=self.batch,drop_remainder=True)\n",
    "\n",
    "       #All the batches will be stored in the cache after the first iteration\n",
    "        dataset.cache()\n",
    "\n",
    "       #shuffeling \n",
    "        dataset=dataset.shuffle(buffer_size=batch)\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "        dataset.prefetch(tf.data.experimental.AUTOTUNE)   \n",
    "\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_tensorflow_dataset(self):\n",
    "        \n",
    "        '''\n",
    "        Create a tensorflow dataset for training and test\n",
    "        '''\n",
    "\n",
    "        #Reading the train test dataframe\n",
    "\n",
    "        train= pd.read_csv(self.path_to_train_csv)\n",
    "        \n",
    "        #shuffle the train and test dataframe\n",
    "        train= train.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        a= np.array([train['audio1_path'].values,train['audio2_path'].values])\n",
    "        b =np.moveaxis(a,0,-1)\n",
    "    \n",
    "        #Creating the train dataset\n",
    "        train_dataset= self.pipeline(b,train['label'].values) \n",
    "        \n",
    "        test= pd.read_csv(self.path_to_test_csv)\n",
    "        test= test.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        a= np.array([test['audio1_path'].values,test['audio2_path'].values])\n",
    "        b =np.moveaxis(a,0,-1)\n",
    "    \n",
    "        #Creating the train dataset\n",
    "        test_dataset= self.pipeline(b,test['label'].values) \n",
    "        \n",
    "        \n",
    "        return train_dataset,test_dataset\n",
    "\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newGPU",
   "language": "python",
   "name": "newgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
